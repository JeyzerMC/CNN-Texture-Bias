{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stylise-Dataset",
      "provenance": [],
      "collapsed_sections": [
        "VAGTXmbOwq9B",
        "lSebTH_wmLA8",
        "tMwexcrWnG9n"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyT_A97VqmJl",
        "colab_type": "text"
      },
      "source": [
        "This notebook is a direct translation of: \n",
        "https://github.com/bethgelab/stylize-datasets\n",
        "to simplify the stylization of images in our use case. \n",
        "\n",
        "We simply combined and adapted their scripts slightly into a runnable notebook to acceleration iterations of our experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNK1hhcd0qEB",
        "colab_type": "text"
      },
      "source": [
        "## Get dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz9ezNVC0uTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('./imagenette2-160.zip'):\n",
        "  import gdown\n",
        "  url = 'https://drive.google.com/uc?id=11MOFZF2dVjEu0PbSPmGVWbb2olSZ8eHV'\n",
        "  output = 'imagenette2-160.zip'\n",
        "  gdown.download(url, output, quiet=True)\n",
        "  !unzip \"imagenette2-160.zip\" -d .\n",
        "  %rm imagenette2-160.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC0jX1gnsEyX",
        "colab_type": "text"
      },
      "source": [
        "## Get textures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ddL5G28sGvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('./best-textures.zip'):\n",
        "  import gdown\n",
        "  url = 'https://drive.google.com/uc?id=1bGHUaL7DzFzVsJKYv04o4ZdNZ9zCTAhY'\n",
        "  output = 'best-textures.zip'\n",
        "  gdown.download(url, output, quiet=True)\n",
        "  !unzip \"best-textures.zip\" -d .\n",
        "  %rm best-textures.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BldM3LYF5rpn",
        "colab_type": "text"
      },
      "source": [
        "## Options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUz6-jY5o8GQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Directory path to a batch of content images\n",
        "# CONTENT_DIR = Path('./imagenette2-160').resolve()\n",
        "CONTENT_DIR = Path('./imagenette2-160').resolve()\n",
        "\n",
        "# Directory path to a batch of style images\n",
        "STYLE_DIR = Path('./best-textures').resolve()\n",
        "\n",
        "# Directory to save the output images\n",
        "OUTPUT_DIR = Path('./stylized-imagenette2-160').resolve()\n",
        "\n",
        "# Number of styles to create for each image\n",
        "NUM_STYLE = 1\n",
        "\n",
        "# The weight that controls the degree of stylization. Should be between 0 and 1.\n",
        "ALPHA = 1.0\n",
        "\n",
        "# List of image extensions to scan style and content directory for.\n",
        "EXTENSIONS = ['JPEG', 'jpg']\n",
        "\n",
        "# New (minimum) size for the content image, keeping the original size if 0\n",
        "CONTENT_SIZE = 0\n",
        "\n",
        "# New (minimum) size for the style image, keeping the original size if 0\n",
        "STYLE_SIZE = 512\n",
        "\n",
        "# CROP SIZE\n",
        "CROP = 0\n",
        "\n",
        "assert CONTENT_DIR.is_dir(), 'Content directory not found'\n",
        "assert STYLE_DIR.is_dir(), 'Style directory not found'\n",
        "assert len(EXTENSIONS) > 0, 'No file extensions specified'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAGTXmbOwq9B",
        "colab_type": "text"
      },
      "source": [
        "## Get models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dGd3MLxvpyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir models\n",
        "%cd models/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv_10JnkwF7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=108uza-dsmwvbW2zv-G73jtVcMU_2Nb7Y' -O vgg_normalised.pth\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1w9r1NoYnn7tql1VYG3qDUzkbIks24RBQ' -O decoder.pth\n",
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSebTH_wmLA8",
        "colab_type": "text"
      },
      "source": [
        "## function.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOgwurKXm97z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "def calc_mean_std(feat, eps=1e-5):\n",
        "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
        "    size = feat.data.size()\n",
        "    assert (len(size) == 4)\n",
        "    N, C = size[:2]\n",
        "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
        "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
        "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
        "    return feat_mean, feat_std\n",
        "\n",
        "# adaptive_instance_normalization\n",
        "def adain(content_feat, style_feat):\n",
        "    assert (content_feat.data.size()[:2] == style_feat.data.size()[:2])\n",
        "    size = content_feat.data.size()\n",
        "    style_mean, style_std = calc_mean_std(style_feat)\n",
        "    content_mean, content_std = calc_mean_std(content_feat)\n",
        "\n",
        "    normalized_feat = (content_feat - content_mean.expand(\n",
        "        size)) / content_std.expand(size)\n",
        "    return normalized_feat * style_std.expand(size) + style_mean.expand(size)\n",
        "\n",
        "\n",
        "def _calc_feat_flatten_mean_std(feat):\n",
        "    # takes 3D feat (C, H, W), return mean and std of array within channels\n",
        "    assert (feat.size()[0] == 3)\n",
        "    assert (isinstance(feat, torch.FloatTensor))\n",
        "    feat_flatten = feat.view(3, -1)\n",
        "    mean = feat_flatten.mean(dim=-1, keepdim=True)\n",
        "    std = feat_flatten.std(dim=-1, keepdim=True)\n",
        "    return feat_flatten, mean, std\n",
        "\n",
        "\n",
        "def _mat_sqrt(x):\n",
        "    U, D, V = torch.svd(x)\n",
        "    return torch.mm(torch.mm(U, D.pow(0.5).diag()), V.t())\n",
        "\n",
        "\n",
        "def coral(source, target):\n",
        "    # assume both source and target are 3D array (C, H, W)\n",
        "    # Note: flatten -> f\n",
        "\n",
        "    source_f, source_f_mean, source_f_std = _calc_feat_flatten_mean_std(source)\n",
        "    source_f_norm = (source_f - source_f_mean.expand_as(\n",
        "        source_f)) / source_f_std.expand_as(source_f)\n",
        "    source_f_cov_eye = \\\n",
        "        torch.mm(source_f_norm, source_f_norm.t()) + torch.eye(3)\n",
        "\n",
        "    target_f, target_f_mean, target_f_std = _calc_feat_flatten_mean_std(target)\n",
        "    target_f_norm = (target_f - target_f_mean.expand_as(\n",
        "        target_f)) / target_f_std.expand_as(target_f)\n",
        "    target_f_cov_eye = \\\n",
        "        torch.mm(target_f_norm, target_f_norm.t()) + torch.eye(3)\n",
        "\n",
        "    source_f_norm_transfer = torch.mm(\n",
        "        _mat_sqrt(target_f_cov_eye),\n",
        "        torch.mm(torch.inverse(_mat_sqrt(source_f_cov_eye)),\n",
        "                 source_f_norm)\n",
        "    )\n",
        "\n",
        "    source_f_transfer = source_f_norm_transfer * \\\n",
        "                        target_f_std.expand_as(source_f_norm) + \\\n",
        "                        target_f_mean.expand_as(source_f_norm)\n",
        "\n",
        "    return source_f_transfer.view(source.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMwexcrWnG9n",
        "colab_type": "text"
      },
      "source": [
        "## net.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bos5GDo9nJ8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "net_decoder = nn.Sequential(\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 128, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 128, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 64, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 64, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 3, (3, 3)),\n",
        ")\n",
        "\n",
        "net_vgg = nn.Sequential(\n",
        "    nn.Conv2d(3, 3, (1, 1)),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(3, 64, (3, 3)),\n",
        "    nn.ReLU(),  # relu1-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 64, (3, 3)),\n",
        "    nn.ReLU(),  # relu1-2\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 128, (3, 3)),\n",
        "    nn.ReLU(),  # relu2-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 128, (3, 3)),\n",
        "    nn.ReLU(),  # relu2-2\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 256, (3, 3)),\n",
        "    nn.ReLU(),  # relu3-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),  # relu3-2\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),  # relu3-3\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),  # relu3-4\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu4-1, this is the last layer used\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu4-2\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu4-3\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu4-4\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu5-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu5-2\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu5-3\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU()  # relu5-4\n",
        ")\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Net, self).__init__()\n",
        "        enc_layers = list(encoder.children())\n",
        "        self.enc_1 = nn.Sequential(*enc_layers[:4])  # input -> relu1_1\n",
        "        self.enc_2 = nn.Sequential(*enc_layers[4:11])  # relu1_1 -> relu2_1\n",
        "        self.enc_3 = nn.Sequential(*enc_layers[11:18])  # relu2_1 -> relu3_1\n",
        "        self.enc_4 = nn.Sequential(*enc_layers[18:31])  # relu3_1 -> relu4_1\n",
        "        self.decoder = decoder\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    # extract relu1_1, relu2_1, relu3_1, relu4_1 from input image\n",
        "    def encode_with_intermediate(self, input):\n",
        "        results = [input]\n",
        "        for i in range(4):\n",
        "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
        "            results.append(func(results[-1]))\n",
        "        return results[1:]\n",
        "\n",
        "    # extract relu4_1 from input image\n",
        "    def encode(self, input):\n",
        "        for i in range(4):\n",
        "            input = getattr(self, 'enc_{:d}'.format(i + 1))(input)\n",
        "        return input\n",
        "\n",
        "    def calc_content_loss(self, input, target):\n",
        "        assert (input.data.size() == target.data.size())\n",
        "        assert (target.requires_grad is False)\n",
        "        return self.mse_loss(input, target)\n",
        "\n",
        "    def calc_style_loss(self, input, target):\n",
        "        assert (input.data.size() == target.data.size())\n",
        "        assert (target.requires_grad is False)\n",
        "        input_mean, input_std = calc_mean_std(input)\n",
        "        target_mean, target_std = calc_mean_std(target)\n",
        "        return self.mse_loss(input_mean, target_mean) + \\\n",
        "               self.mse_loss(input_std, target_std)\n",
        "\n",
        "    def forward(self, content, style):\n",
        "        style_feats = self.encode_with_intermediate(style)\n",
        "        t = adain(self.encode(content), style_feats[-1])\n",
        "\n",
        "        g_t = self.decoder(Variable(t.data, requires_grad=True))\n",
        "        g_t_feats = self.encode_with_intermediate(g_t)\n",
        "\n",
        "        loss_c = self.calc_content_loss(g_t_feats[-1], t)\n",
        "        loss_s = self.calc_style_loss(g_t_feats[0], style_feats[0])\n",
        "        for i in range(1, 4):\n",
        "            loss_s += self.calc_style_loss(g_t_feats[i], style_feats[i])\n",
        "        return loss_c, loss_s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3LMdNZYoWfh",
        "colab_type": "text"
      },
      "source": [
        "## stylise.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0klNyFVoal_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm\n",
        "\n",
        "def input_transform(size, crop):\n",
        "    transform_list = []\n",
        "    if size != 0: \n",
        "        transform_list.append(torchvision.transforms.Resize(size))\n",
        "    if crop != 0:\n",
        "        transform_list.append(torchvision.transforms.CenterCrop(crop))\n",
        "    transform_list.append(torchvision.transforms.ToTensor())\n",
        "    transform = torchvision.transforms.Compose(transform_list)\n",
        "    return transform\n",
        "\n",
        "def style_transfer(vgg, decoder, content, style, alpha=1.0):\n",
        "    assert (0.0 <= alpha <= 1.0)\n",
        "    content_f = vgg(content)\n",
        "    style_f = vgg(style)\n",
        "    feat = adain(content_f, style_f)\n",
        "    feat = feat * alpha + content_f * (1 - alpha)\n",
        "    return decoder(feat)\n",
        "\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "def extract_cifar(path):\n",
        "    dataset = []\n",
        "    files = list(path.rglob('*'))\n",
        "    for file in files:\n",
        "        dataset.append(unpickle(file))\n",
        "    return dataset\n",
        "\n",
        "def main():\n",
        "    dataset = []\n",
        "    for ext in EXTENSIONS:\n",
        "        dataset += list(CONTENT_DIR.rglob('*.' + ext))\n",
        "\n",
        "    assert len(dataset) > 0, 'No images with specified extensions found in content directory' + CONTENT_DIR\n",
        "    content_paths = sorted(dataset)\n",
        "    print('Found %d content images in %s' % (len(content_paths), CONTENT_DIR))\n",
        "\n",
        "    # collect style files\n",
        "    styles = []\n",
        "    for ext in EXTENSIONS:\n",
        "        styles += list(STYLE_DIR.rglob('*.' + ext))\n",
        "\n",
        "    assert len(styles) > 0, 'No images with specified extensions found in style directory' + STYLE_DIR\n",
        "    styles = sorted(styles)\n",
        "    print('Found %d style images in %s' % (len(styles), STYLE_DIR))\n",
        "\n",
        "    decoder = net_decoder\n",
        "    vgg = net_vgg\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    decoder.eval()\n",
        "\n",
        "    vgg.eval()\n",
        "\n",
        "    decoder.load_state_dict(torch.load('models/decoder.pth'))\n",
        "    vgg.load_state_dict(torch.load('models/vgg_normalised.pth'))\n",
        "    vgg = nn.Sequential(*list(vgg.children())[:31])\n",
        "\n",
        "    vgg.to(device)\n",
        "    decoder.to(device)\n",
        "\n",
        "    content_tf = input_transform(CONTENT_SIZE, CROP)\n",
        "    style_tf = input_transform(STYLE_SIZE, 0)\n",
        "\n",
        "\n",
        "    # disable decompression bomb errors\n",
        "    Image.MAX_IMAGE_PIXELS = None\n",
        "    skipped_imgs = []\n",
        "    \n",
        "    # actual style transfer as in AdaIN\n",
        "    with tqdm(total=len(content_paths)) as pbar:\n",
        "        for content_path in content_paths:\n",
        "            try:\n",
        "                content_img = Image.open(content_path).convert('RGB')\n",
        "                for style_path in random.sample(styles, NUM_STYLE):\n",
        "                    style_img = Image.open(style_path).convert('RGB')\n",
        "\n",
        "                    content = content_tf(content_img)\n",
        "                    style = style_tf(style_img)\n",
        "                    style = style.to(device).unsqueeze(0)\n",
        "                    content = content.to(device).unsqueeze(0)\n",
        "                    with torch.no_grad():\n",
        "                        output = style_transfer(vgg, decoder, content, style,\n",
        "                                                ALPHA)\n",
        "                    output = output.cuda()\n",
        "\n",
        "                    rel_path = content_path.relative_to(CONTENT_DIR)\n",
        "                    out_dir = OUTPUT_DIR.joinpath(rel_path.parent)\n",
        "\n",
        "                    # create directory structure if it does not exist\n",
        "                    if not out_dir.is_dir():\n",
        "                        out_dir.mkdir(parents=True)\n",
        "\n",
        "                    content_name = content_path.stem\n",
        "                    style_name = style_path.stem\n",
        "                    out_filename = content_name + '-stylized-' + style_name + content_path.suffix\n",
        "                    output_name = out_dir.joinpath(out_filename)\n",
        "\n",
        "                    save_image(output, output_name, padding=0) #default image padding is 2.\n",
        "                    style_img.close()\n",
        "                content_img.close()\n",
        "            except OSError as e:\n",
        "                print('Skipping stylization of %s due to an error' %(content_path))\n",
        "                skipped_imgs.append(content_path)\n",
        "                continue\n",
        "            except RuntimeError as e:\n",
        "                print('Skipping stylization of %s due to an error' %(content_path))\n",
        "                skipped_imgs.append(content_path)\n",
        "                continue\n",
        "            finally:\n",
        "                pbar.update(1)\n",
        "            \n",
        "    if(len(skipped_imgs) > 0):\n",
        "        with open(OUTPUT_DIR.joinpath('skipped_imgs.txt'), 'w') as f:\n",
        "            for item in skipped_imgs:\n",
        "                f.write(\"%s\\n\" % item)\n",
        "main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8KkPhvLLHkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r ./output.zip ./output"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}