%%%% ijcai19.tex

\typeout{IJCAI-19 Instructions for Authors}

% These are the instructions for authors for IJCAI-19.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai19.sty is NOT the same than previous years'
\usepackage{ijcai19}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{epstopdf}
\usepackage{makecell}
\usepackage{float}
\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

\title{ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness}

% Single author syntax
\iffalse
\author{
    Sarit Kraus
    \affiliations
    Department of Computer Science, Bar-Ilan University, Israel \emails
    pcchair@ijcai19.org
}
\fi

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% Check the ijcai19-multiauthor.tex file for detailed instructions

\author{
Andre Diler$^1$
\and
Mehdi Chaid$^1$\and
Abderahmane Bouziane$^1$\
\affiliations
$^1$Département GIGL Polytechnique Montreal\\
\emails
andre.diler@polymtl.ca,
mehdi.chaid@polymtl.ca,
abderahmane.bouziane@polymtl.ca
}

\begin{document}

\maketitle

\begin{abstract}
The history of computer vision led us to believe that like humans, 
Convolutional Neural Networks (CNNs) would recognize objects mainly by their shapes.
Recent studies in the field however, have  suggested that CNNs rely more on image textures 
rather than edges and shapes in order to perform object detection. 
A paper published in November 2018, by \cite{geirhos2018imagenettrained}, explored the idea of texture bias
and presented novels solutions in order to shift the trend towards a stronger shape bias for CNNs, similar to 
how humans perceive things.
The following report attempts to analyse the hypothesis proposed in the paper, 
as well as offer a scopped reproduction of the experiments conducted by the authors, in a smaller environment, 
in order to draw new conclusions and reinforce our understanding of the internals of CNNs.
\end{abstract}

\section{Introduction}

% https://hackernoon.com/a-brief-history-of-computer-vision-and-convolutional-neural-networks-8fe8aacc79f3

Modern Convolutional Neural Networks regularly reach very high performances on complex computer vision tasks 
such as image classification and segmentation.
These performances are reaching human levels in term of accuracy, and come from a long history of studies 
on human and machine perception. \medskip \par 

\noindent
As such, and reinforced with other experiments on the matter, 
it is commonly believed that CNNs learn features from the shapes during the training phase
and use these for detection.

\subsection{Related Work}

\noindent
The first proof that shapes are fundamental in computer vision came from a very influential study on cognition, 
by \cite{hubel1959receptive}, who described how biological neurons could extract features from images,
amongst which certain types of neurons were activated specifically by edges. \medskip \par 

\noindent 
Another paper from \cite{marr1982vision} concluded that vision was hierachical. Low-level features, such as lines,
were combined together to recognize more high level concepts, 
like wheels, windows, etc, and form objects. \medskip \par 

\noindent
The famous Neocognition paper \cite{neocognitron} was 
the implementation that introduced the idea of hierachical vision. 

\noindent
The multilayered neural network they proposed 
included multiple convolutional layers with wheighted receptive fields (filters).
It was the first known deep neural network. \medskip \par

\noindent
However, the first modern convnet was LeNet \cite{Lecun98gradient-basedlearning}.
This CNN used backpropagation to automatically learn the filter values to extract meaningful 
features in images hierachically.
Nowadays, all the recent convolutional neural networks are inspired from this network. \medskip \par

\noindent
ConvNet were considered like a black box for a long time, 
hence why extensive efforts were conducted during recent years in order 
to analyze the internals of these networks. \medskip \par

\noindent
The shape hypothesis that emerged from early experiments consists in
"High-level units appear to learn representations of shapes occurring in natural images” 
\cite{Kriegeskorte029876}. This theory quickly became widespread in the community, and
is understandable given the history of computer vision. \medskip \par

\noindent
Furthermore, there are a lot of studies comparing human vision with computer vision. For example,
\cite{kubiliusshape} stated that 
"implicitly learn representations of shape that reflect human shape perception"
while a paper from \cite{ritter2017cognitive} concluded that 
"state-of-the-art one-shot learning models trained on ImageNet exhibit  
a similar bias to that observed in humans: they prefer to categorize objects according  
to shape rather than color" . \medskip \par

\noindent
However, several researchers raised doubts about the shape hypothesis.
According to studies from \cite{gatys2017textures} and \cite{brendel2019approximating}, 
CNNs are able to classify texturised images even if their shape structure is destroyed.
Moreover, the same paper from \cite{brendel2019approximating} also
showed that CNNs with constrained receptive field sizes can reach competitive accuracies on ImageNet.
It is also worth noticing that small receptive fields cannot capture the overal shape of an image.
These results have led the authors of the paper we've analysed, \cite{geirhos2018imagenettrained}, 
to emit a new hypothesis: the texture hypothesis, where "in contrast to the
common assumption, object textures are more important than global object shapes for CNN object
recognition". \medskip \par

\noindent
Our objective in this report is to submit those hypothesis to a test through diverse experiments 
and validate in our own environment the results obtained by the original authors.

\newpage
\section{Methodology}

\subsection{Dataset}

We used Imagenette \cite{fastai2019}, a subset of the Imagenet dataset created 
by fastai to conduct our experiments.
This ensured that our results would be comparable to the authors, 
while keeping an acceptable size for the dataset given the time and material constraints.
Imagenette contains 13394 images.
Imagenette's train/test split is 70/30: 9469 images for the train and validation set, and 3925 images for the test set.
It contains 10 easily classified classes from Imagenet, namely
tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute)
The classes are balanced with around 950 images in each one.

This dataset is more easily classifiable than Imagenet in less time, which fits our purpose.
We used the '160px' version of the images, where the shortest size of the images are resized to that size, 
with their aspect ratio maintained.


\subsection{Style Transfer}

This procedure ensures that the images conserve their edges, but not their textures.
In order to destroy all the texture information of an image without its edges, we used style transfer like in
the original paper.
However, we generated our own Stylized-Dataset. \smallskip

\noindent
The image for which you want to change the texture, but keep the edges is called the content image.
The image were the the texture is extracted is called the texture image.
The image created by the style transfer is called the stylized image. \smallskip

\noindent
We applied style transfer on Imagenette to replace the original texture of the object (dog's fur for example) 
with a random texture (a parachute's fabric and color) 
from sampled from the Describable Texture Dataset \cite{cimpoi14describing}. \smallskip

\noindent
Our first aproach was to use a random image from the Texture Dataset to stylize Imagenette. However, 
the edges of the images were sometimes destroyed, and were not identifiable by humans. \smallskip

\noindent
To avoid poor quality images, we handpicked 11 texture images that generated good quality stylized images.
Each time an Imagenette image was to be stylized, we picked a texture image randomly from our 11 textures.
This ensured that the texture of a stylized image had no correlation with its original class. \smallskip

\noindent
We used the implementation of AdaIN-style \cite{huang2017arbitrary} made by \cite{stylizeddatasets2019}.
\begin{figure}[h!]\center
  \includegraphics[width=0.45\textwidth]{imgs/adain_architecture}
  \caption{AdaIn style transfer network.}
\end{figure}

AdaIN-style is the first real-time style transfer algorithm than can transfer arbitrary new styles.
That means that we can stylize an image with any texture image we want.
This pretrained architecture uses a VGG-19 network to encode the content and style images.
An AdaIN layer is then used to perform style transfer in the feature space.   
A Decoder then decodes the AdaIN output to image space.

\subsection{Metrics}

We decided to only use Accuracy, as our class are balanced, and not too numerous.

Top-k accuracy is a useful metric when there is a lot of classes to predict. Our dataset only
contains 10 classes, so we deemed it wasn't necessary to introduce this metric.

\subsection{ResNet}

TODO

\subsubsection{Loss Function}

\subsubsection{Optimisation Function}


\subsection{Other models ???}

TODO MAYBE ?

\subsection{Cue Conflict ???}

TODO MAYBE ?

\newpage
\section{Experiments}

For all , SIN and IN represent Stylized-Imagenette and Imagenette datasets respectively.
For example, the nomenclature IN \texorpdfstring{\textrightarrow} .IN signifies that the 
model was trained on IN and tested on IN.
IN and SIN have the same image ids for their testing set, only the stylization changes.

\subsection{Texture bias hypothesis}
The first experiment conducted by the authors was to validate the texture bias 
through image classification on handpicked stylized subsets. 
Their hypothesis was that the re-texturized image would be classified as the texture representation, rather than 
the underlying content using the shapes, which would be contrary to popular belief. \smallskip

\noindent
We've reproduced their experiment using the picture of an English Springer as our content image (a), 
a colorful parachute as our texture image (b),
and a stylized version of the Springer (c) using AdaIN style transfer 
(Huang \& Belongie, 2017) % TODO: ADD REFERENCE TO STYLE TRANSFER
to introduce a texture-shape cue conflicts. \smallskip

\noindent
The classification results after training the resnet-18 on the imagenette-160 dataset 
can be seen on figure \ref{dog-parachute}.
We've obtained probabities in line with what was suggested by the authors, 
where the stylized image was recognized as a parachute, 
due to the texture bias, rather than an English Springer.

\begin{figure}[h!]\center
  \includegraphics[width=0.47\textwidth]{imgs/results-textures}
  \caption{Classification results after training on imagenette-160.}
  \label{dog-parachute}
\end{figure}

\noindent
Retraining the model on the stylized imagenette-160 dataset with the same parameters 
(resnet-18, 20 epochs, 2e-2 learning rate), 
led to classification probabities available in figure \ref{stylized-dog-parachute}.
As expected, and although the results are a bit less accurate than on the original model, 
we get the correct class prediction for the stylized image, suggesting that our CNN recognized the object
from its shape this time around, having no texture clue to learn from.
\textbf{TODO: FINISH FIGURE 2 AND PUT HERE}

\begin{figure}[h!]\center
  \includegraphics[width=0.47\textwidth]{imgs/results-textures}
  \caption{Classification results with stylized imagenette-160.}
  \label{stylized-dog-parachute}
\end{figure}

\noindent
The results of this experiment are also available in the dog-parachute notebook, 
available on the github repository, under the section Test-Visualisation.
\textbf{TODO: ADD LINK TO GITHUB EXPERIMENT FOR ABOVE.}

\subsection{Training Procedure}


\subsubsection{Global Parameters}

\textit{Number of epochs}: For our experiments, we did a fixed number of epochs
 (15) because it did not overfit, nor underfit the data too much (Fig. \ref{loss_in}).

 \begin{figure}[h!]
  \includegraphics[width = 0.45\textwidth]{imgs/sin/loss}
  \caption{Loss of training on IN \texorpdfstring{\textrightarrow} .IN}
  \label{loss_in}
\end{figure}

\textit{Learning rate}: We used FastAI's "lr finder" (Fig. \ref{lr_finder}) to tune our learning rate.
We fixed the maximum learning rate at 1e-2 by looking at the learning rate space search graph.
\begin{figure}[h!]
  \includegraphics[width = 0.45\textwidth]{imgs/lr_find.eps}
  \caption{Learning rate finder IN \texorpdfstring{\textrightarrow} .IN}
  \label{lr_finder}
\end{figure}

\subsubsection{Experiment-specific parameters}
We used Early Stopping for the FineTuning experiment only, because this model was more prone to overfitting.
As we can see in Fig. \ref{lr_finetune} , the maximum learning rate has to be much lower than previous experiments 
We defined the max learning rate at 1e-4.

\begin{figure}[h!]
  \includegraphics[width = 0.45\textwidth]{imgs/lr_finetune.png}
  \caption{Learning rate finder of SIN + IN + finetune(IN)\texorpdfstring{\textrightarrow} .IN}
  \label{lr_finetune}
\end{figure}


\subsection{Robustness of shape-based representations}

We reproduce the 4 experiments of the original paper to prove the robustness of a shape-biased CNN
(the CNN trained on SIN),
and show the fragility of the texture biased CNN (the CNN trained on IN). 

\begin{table}[h!]
  \begin{tabular}{lllll}
  \Xhline{2\arrayrulewidth}
  Architecture & IN\texorpdfstring{\textrightarrow} .IN & SIN\texorpdfstring{\textrightarrow} .SIN & IN\texorpdfstring{\textrightarrow} .SIN & SIN\texorpdfstring{\textrightarrow}.IN \\ \hline
  ResNet-18    & \textbf{0.963}     & 0.908      & 0.313     & 0.624     \\ \Xhline{2\arrayrulewidth}
  \end{tabular}
  \caption{Accuracy of ResNet18 model on different tests}
\end{table}


\subsubsection{IN \texorpdfstring{\textrightarrow} .IN}
A model trained on IN and tested on IN is the usual training scheme used in the literature.
It yields competitive results, as we already know (Fig. \ref{cm_in-in}).

\begin{figure}[h!]
  \includegraphics[width = 0.45\textwidth]{imgs/in/in-in/in-in_confusion_matrix_0.963.eps}
  \caption{Confusion Matrix of IN \texorpdfstring{\textrightarrow} .SIN}
  \label{cm_in-in}
\end{figure}

\subsubsection{IN \texorpdfstring{\textrightarrow} .SIN}



The original paper trained a model on IN and tested it on SIN which yielded poor results (Fig. \ref{cm_in-sin}) as 
the model is texture biased, and the SIN dataset is specifically designed to promote shape biased models.

\begin{figure}[h!]
  \includegraphics[width = 0.45\textwidth]{imgs/in/in-sin/in-sin_confusion_matrix_0.313.eps}
  \caption{Confusion Matrix of IN \texorpdfstring{\textrightarrow} .SIN}
  \label{cm_in-sin}
\end{figure}

\subsubsection{SIN \texorpdfstring{\textrightarrow} .SIN}
This training scheme yields fairly good results (Fig. \ref{cm_sin-sin}). This shows that the model is capable to learn
and generalize on the stylized dataset.

\begin{figure}[h!]
  \includegraphics[width = 0.45\textwidth]{imgs/sin/sin-sin/sin-sin_confusion_matrix_0.908.eps}
  \caption{Confusion Matrix of SIN \texorpdfstring{\textrightarrow} .SIN}
  \label{cm_sin-sin}
\end{figure}

\subsubsection{SIN \texorpdfstring{\textrightarrow} .IN}

This training scheme yields superior results (Fig. \ref{cm_sin-in}) than IN \texorpdfstring{\textrightarrow} .SIN (Fig. \ref{cm_in-sin}), however
there are much better than IN \texorpdfstring{\textrightarrow} .SIN.
This demonstrates that a shape-biased model generalizes better than texture biased models.

\begin{figure}[h!]
  \includegraphics[width = 0.45\textwidth]{imgs/sin/sin-in/sin-in_confusion_matrix_0.624.eps}
  \caption{Confusion Matrix of SIN \texorpdfstring{\textrightarrow} .IN}
  \label{cm_sin-in}
\end{figure}

\subsection{Shape-ResNet}

Now that we demonstrated the excellent generalization capabilites of shape biased models,
our goal is to surpass the performance of the IN \texorpdfstring{\textrightarrow} .IN model.
\textbf{All the following models are evaluated on the IN dataset.}

\begin{table}[h!]
  \begin{tabular}{lllll}
  \Xhline{2\arrayrulewidth}
  Architecture & IN & SIN+SIN  & SIN + IN + finetune(IN) \\ \hline
  ResNet-18    & 0.963    & 0.973   & \textbf{0.974}    \\ \Xhline{2\arrayrulewidth}
  \end{tabular}
  \caption{Accuracy of ResNet18 model on IN test set}
\end{table}


\subsubsection{SIN + IN \texorpdfstring{\textrightarrow} .IN}

Mixing the 2 datasets has the effect of a data augmentation. The model will be better
at generalizing (Fig. \ref{cm_sinin-in}). The performance improvement as a result of data augmentation 
is well known. However, this data augmentation is dedicated to increase shape bias,
and does not make the model overfit.

\begin{figure}[h!]
  \includegraphics[width = 0.45\textwidth]{imgs/sinin/sinin-in_confusion_matrix_0.973.eps}
  \caption{Confusion Matrix of SIN + IN \texorpdfstring{\textrightarrow} .IN}
  \label{cm_sinin-in}
\end{figure}

\subsubsection{SIN + IN + finetune(IN) \texorpdfstring{\textrightarrow} .IN}

This method yields the best results (Fig. \ref{cm_sinin-in-finetune}) The finetuning only occurs
on the last layers of the ResNet (the classification layers) and not the feature extracting layers.
Hence, the model does not overfit on textures, but is better to better classify images.

\begin{figure}[h!]
  \includegraphics[width = 0.45\textwidth]{imgs/sinin/finetune/fine_tune_confusion_matrix_0.974.eps}
  \caption{Confusion Matrix of SIN + IN + finetune(IN) \texorpdfstring{\textrightarrow} .IN}
  \label{cm_sinin-in-finetune}
\end{figure}



We can see on the Fig. \ref{loss_finetune} that the train loss is fluctuating a lot even with a small
learning rate. However, the validation loss is stable and slowly decreasing.
The validation loss is lower than the training loss, which is a sign of underfitting. This is what we want:
the feature extraction is more shape biased than texture biased, and the model is not overfitting on the texture like
before.

\begin{figure}[h!]
  \includegraphics[width = 0.45\textwidth]{imgs/sinin/finetune/loss.eps}
  \caption{Loss of training on the finetuning experiment}
  \label{loss_finetune}
\end{figure}

\subsection{Noise resistance}


One of the major finding in the original paper was that the model trained with a shape bias had developed a better resistance to distortions than the regular model.
A model trained to recognize textures should behave differently from one trained to recognize shapes on a noised data set.  The key idea is that not all types of distortions are the same and this model should in theory only be resistant to some of them.  We decided to focus on 3 main types of distortions.  A uniform noise, a high pass filter and a low pass filter.

The result of the original paper show us that the SIN model has a better resistance to uniform noise, high pass filtered image and a worse resistance to low pass filtered images.
This makes sense since a uniform noise has a much worse effect on texture than overall shape. Also, a high pass filter acts as a contour amplifier thus making the shape in the image more prominent. In the same line of thought, a low pass filter has the opposite effect, blurring the edges of an image and making it's shape much less discernible.

\begin{figure}[h!]
\centering
\includegraphics[width = 0.25\textwidth]{imgs/image_uniform}
\caption{uniform noise}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width = 0.25\textwidth]{imgs/image_high}
\caption{high pass}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width = 0.25\textwidth]{imgs/image_low}
\caption{low pass}

\end{figure}

\subsubsection{experiment}

We decided to reproduce this experiment with our own data.
All of the distorted data sets are created form the regular imagenette data set. We applied the distortions at different scale gradually increasing the effect on the images.

We then compared the results of our three trained models against those different noise levels.
The models were the regular resnet trained on imagenette (IN), a resnet trained on a stylised version of imagenette (SIN) and one trained on a combination of this data (SIN+IN).
Each model was given the same test set on which a gradually increasing noise was applied and an accuracy was computed.

The different level of noise were chosen arbitrarily to create different enough distortions. The original values and algorithms used in the paper were not available.

\subsubsection{results}

In all of the test we conducted, the SIN model always start off worse. This is to be expected because it was trained on stylised images and the test set for the distorted images is based on regular images. The IN and SIN+IN model start at about the same accuracy.

\begin{figure}[h!]
\centering
\includegraphics[width = 0.45\textwidth]{imgs/uniform}
\caption{Uniform noise}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width = 0.45\textwidth]{imgs/high_pass}
\caption{High noise}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width = 0.45\textwidth]{imgs/low_pass}
\caption{Low pass}
\end{figure}

The figures clearly show the shape biased models have a better resistance to noise the regular model.

We were surprised by the results for two reasons.
First, the SIN model started off at a worse accuracy, so we were expecting to compare the resistance to distortions as a relative drop from the original accuracy but it managed to have a better absolute accuracy than the regular model.
Secondly, we were expecting the SIN model to under perform in the low pass area, but it managed to be better than the IN model. This goes against the results of the original paper and against common sense. It is possible the levels of blur applied were either too strong or not enough

\section{Approach analysis}

\subsection{Dataset}

As mentionned in the first section, we've used a subset of 
the same dataset the authors used in their experiment. 
Indeed, we didn't have much time to find another image classification dataset that fitted our needs.
Many of the datasets we found online were poorly anotated, or the images
were too large. \smallskip

\noindent
The original Imagenet dataset had too many classes and a large number of images (over 14M), 
which would take way too long to train. 
We also did not have the physical storage capacities for this dataset. \smallskip

\noindent
On the other hand, CIFAR-10's images quality was too poor 
to obtain nice results on our style transfer procedure. \smallskip

\noindent
We've also experimented quickly with the Natural Images dataset \cite{natural-images}, 
but promising results that were obtained in parallele using the smaller version of imagenette, 
imagenette-160, led us to focus on this dataset for the rest of our experiments. \smallskip

\noindent
It would be interesting, as a next step, to test this approach on the Natural Images dataset,
or completely different dataset altogether and validate the results on those ones.

% TODO
%  (include figure in /imgs/dataset_comparison)

\subsection{Results}

Overall, we were able to reproduce their results and
even surpass them. This can be justified by the fact that the classes
in Imagenette were handpicked to be easily classified, and that there were less 
classes than Imagenet.
Our results fluacted a bit between different training. For example, finetuning does not seem to be necessary 
in our study. However, when the performance of SIN + IN is sometimes lower than 0.9. In this scenario, finetuning
always surpassed IN \texorpdfstring{\textrightarrow} .IN.
To correct this, we could have calculated our accuracy on the average of five runs, like the auhors of Imagenette \cite{fastai2019}
suggested.

\subsection{Experiments}

We only reproduced experiments that yielded interesting results 
in the original paper. We can see the excellent quality of the work, 
because their experiments were easily reproducable, and most of the 
steps were clearly explained.

\subsection{Ideas for future studies}

Testing this robustness in related areas were shape bias (like image segmentation) 
is more important than image classification could be interesting.

\section{Conclusion}

This is where we conclude bois hjf

% To begin with, we reproduced the first experiment of the paper. This experiment
% is represents the misconception we have about how CNNs learn.

% Figure Mehdi

% We can see that the pretrained CNN recognized the elephant with just its texture.
% More interestingly, it recognized it classified the cat shape with elephant texture as an elephant.
% The texture hypothesis is more likely to be true.
% Let's define more robust experiments to verify that.

% \newpage
% \subsection{Texture bias of CNNs}

% General explanation

% \subsubsection{IN to IN}
% expected results
% \subsubsection{SIN to IN}
% expected results

% \subsection{Dataset}

% Details on dataset creation

% \subsection{Resistance to noise}

% Experiences Abder
% Data generation

% \subsection{Training}

% Details of ResNet model
% Hyperparams
% Nb Epochs
% FineTuning method

% \section{Results}

% \subsection{Texture bias of CNNs}

% \subsection{Resistance to noise}


% \section{Discussion}

% \subsection{Texture bias of CNNs}

% \subsection{Resistance to noise}

\appendix

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai19}

\end{document}

